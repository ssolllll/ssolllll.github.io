<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ssolllll.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ssolllll.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-23T12:10:06+00:00</updated><id>https://ssolllll.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[MCP] 1. AI 시대의 USB-C, Model Context Protocol 완벽 이해</title><link href="https://ssolllll.github.io/blog/2025/mcp-basic-concepts/" rel="alternate" type="text/html" title="[MCP] 1. AI 시대의 USB-C, Model Context Protocol 완벽 이해"/><published>2025-12-22T10:00:00+00:00</published><updated>2025-12-22T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/mcp-basic-concepts</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/mcp-basic-concepts/"><![CDATA[<p>지금까지 LLM 애플리케이션을 개발하면서 우리는 수많은 “커넥터(Connector)”를 만들어야 했다. Slack을 연결하려면 Slack API 어댑터를, PostgreSQL을 연결하려면 SQL 쿼리 도구를, Google Drive를 연결하려면 또 별도의 파서를 작성해야 했다.</p> <p>새로운 모델(Claude, GPT-4, Gemini)이 나올 때마다, 혹은 새로운 IDE(Cursor, VS Code)나 챗봇 UI가 나올 때마다 이 연결 작업을 반복해야 한다. 이것이 바로 <strong>“M x N 연결 문제”</strong>다.</p> <p>Anthropic이 오픈소스로 공개한 <strong>MCP(Model Context Protocol)</strong>는 이 난장판을 정리하기 위한 <strong>“AI 시대의 USB-C”</strong> 표준이다.</p> <hr/> <h2 id="1-the-problem-파편화된-연결-fragmentation">1. The Problem: 파편화된 연결 (Fragmentation)</h2> <p>기존 방식에서는 AI 모델이 데이터 소스에 접근하기 위해 각기 다른 전용 연동 코드가 필요했다.</p> <ul> <li><strong>Claude Desktop</strong>에서 로컬 파일을 보려면? -&gt; 전용 기능 필요</li> <li><strong>Cursor</strong>에서 Postgres를 조회하려면? -&gt; 전용 플러그인 필요</li> <li><strong>LangChain</strong> 에이전트가 Slack을 읽으려면? -&gt; LangChain Tool 필요</li> </ul> <p>시스템이 복잡해질수록 유지보수 비용은 기하급수적으로 늘어난다.</p> <h2 id="2-the-solution-mcp-아키텍처">2. The Solution: MCP 아키텍처</h2> <p>MCP는 <strong>Client(호스트) - Server(데이터 제공자)</strong> 구조의 개방형 프로토콜이다. 한 번만 MCP 호환 서버(Server)를 만들면, MCP를 지원하는 모든 클라이언트(Client)에서 별도 수정 없이 해당 데이터를 갖다 쓸 수 있다.</p> <h3 id="핵심-컴포넌트">핵심 컴포넌트</h3> <ol> <li><strong>MCP Host (Client):</strong> LLM을 구동하는 애플리케이션 (예: Claude Desktop, Cursor, Zed, 혹은 우리가 만든 커스텀 챗봇).</li> <li><strong>MCP Server:</strong> 실제 데이터나 기능을 제공하는 주체 (예: Google Drive Server, Postgres Server, Local Filesystem Server).</li> <li><strong>Resources:</strong> 서버가 제공하는 읽기 전용 데이터 (파일, DB 레코드 등).</li> <li><strong>Tools:</strong> 모델이 실행할 수 있는 함수 (API 호출, 쿼리 실행 등).</li> <li><strong>Prompts:</strong> 서버가 제공하는 사전 정의된 프롬프트 템플릿.</li> </ol> <hr/> <h2 id="3-왜-엔지니어에게-중요한가">3. 왜 엔지니어에게 중요한가?</h2> <h3 id="a-한-번-작성하여-어디서나-사용-write-once-run-anywhere">A. 한 번 작성하여 어디서나 사용 (Write Once, Run Anywhere)</h3> <p>PostgreSQL DB를 조회하는 MCP 서버를 하나 파이썬으로 짜두면, 이를 <strong>Claude Desktop</strong>에서도 쓰고, 팀 동료의 <strong>IDE</strong>에서도 쓰고, 사내 <strong>Admin 대시보드</strong>에서도 쓸 수 있다. 클라이언트마다 로직을 다시 짤 필요가 없다.</p> <h3 id="b-로컬-우선-local-first--보안">B. 로컬 우선 (Local First) &amp; 보안</h3> <p>MCP는 기본적으로 로컬 프로세스 간 통신(Stdio)을 지원한다. 내 컴퓨터에 있는 민감한 파일이나 DB 접속 정보를 클라우드에 올리지 않고, 내 로컬에서 돌아가는 LLM Client와 직접 연결할 수 있다.</p> <h3 id="c-생태계의-확장">C. 생태계의 확장</h3> <p>이미 Google Drive, Slack, Git, Postgres 등 주요 서비스에 대한 MCP Server 구현체들이 쏟아져 나오고 있다. 우리는 바퀴를 다시 발명할 필요 없이, 표준 규격에 맞는 서버를 가져다 쓰기만 하면 된다.</p> <hr/> <h2 id="결론">결론</h2> <p>MCP는 단순한 라이브러리가 아니라 <strong>프로토콜(Protocol)</strong>이다. 웹이 HTTP라는 프로토콜 위에서 폭발적으로 성장했듯, AI 에이전트 시장도 MCP라는 표준 위에서 데이터 연결의 장벽을 허물고 폭발할 준비를 하고 있다.</p> <p>다음 포스트에서는 Python을 사용하여 <strong>실제 작동하는 나만의 MCP 서버</strong>를 5분 만에 구축하는 과정을 다룬다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="mcp"/><category term="llm"/><category term="architecture"/><category term="standard"/><category term="anthropic"/><summary type="html"><![CDATA[Anthropic이 공개한 MCP의 등장 배경과 아키텍처 분석. M x N 연결 문제를 M + N으로 해결하는 표준 인터페이스의 혁명.]]></summary></entry><entry><title type="html">[MCP] 2. Python으로 5분 만에 나만의 MCP Server 만들기</title><link href="https://ssolllll.github.io/blog/2025/mcp-server-tutorial/" rel="alternate" type="text/html" title="[MCP] 2. Python으로 5분 만에 나만의 MCP Server 만들기"/><published>2025-12-22T10:00:00+00:00</published><updated>2025-12-22T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/mcp-server-tutorial</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/mcp-server-tutorial/"><![CDATA[<p>이전 글에서 MCP의 개념을 익혔다면, 이제 손을 더럽힐 차례다. 이번 포스트에서는 Python의 <code class="language-plaintext highlighter-rouge">mcp</code> SDK를 사용하여, 로컬 SQLite 데이터베이스를 조회하고 수정하는 간단한 <strong>MCP Server</strong>를 구현해 본다.</p> <p>이 서버를 만들면, <strong>Claude Desktop 앱</strong>이 내 로컬 DB를 자유자재로 조회하여 답변할 수 있게 된다.</p> <hr/> <h2 id="1-환경-설정">1. 환경 설정</h2> <p>MCP 서버 구축을 도와주는 <code class="language-plaintext highlighter-rouge">mcp</code> 패키지를 설치한다. (또는 <code class="language-plaintext highlighter-rouge">fastmcp</code> 같은 래퍼를 쓸 수도 있지만, 원리 이해를 위해 정석 SDK를 사용한다.)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>mcp
</code></pre></div></div> <ol> <li>Server 코드 작성 (weather_server.py) 가장 간단한 예제로, 날씨 정보를 조회하는 기능을 가진 서버를 만들어보자.</li> </ol> <p>Python</p> <p>from typing import Any import httpx from mcp.server.fastmcp import FastMCP</p> <h2 id="1-서버-인스턴스-생성">1. 서버 인스턴스 생성</h2> <p>mcp = FastMCP(“Weather Demo”)</p> <h2 id="2-tool-정의-모델이-호출할-함수">2. Tool 정의 (모델이 호출할 함수)</h2> <ul> <li>데코레이터만 붙이면 자동으로 MCP Tool로 등록된다. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@mcp.tool</span><span class="p">()</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
  <span class="sh">"""</span><span class="s">도시 이름을 입력받아 현재 날씨를 반환합니다.</span><span class="sh">"""</span>
  <span class="c1"># 실제로는 외부 API를 호출하겠지만, 여기선 Mocking
</span>  <span class="n">weather_data</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">seoul</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">맑음, 15도</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">busan</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">흐림, 18도</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">new york</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">비, 10도</span><span class="sh">"</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">weather_data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">city</span><span class="p">.</span><span class="nf">lower</span><span class="p">(),</span> <span class="sh">"</span><span class="s">알 수 없는 도시입니다.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h2 id="3-resource-정의-모델이-읽을-수-있는-데이터">3. Resource 정의 (모델이 읽을 수 있는 데이터)</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@mcp.resource</span><span class="p">(</span><span class="sh">"</span><span class="s">weather://list</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">list_cities</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">날씨 조회가 가능한 도시 목록을 반환합니다.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="sh">"</span><span class="s">Seoul, Busan, New York</span><span class="sh">"</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Stdio 방식으로 실행 (Claude Desktop과 통신하는 표준 방식)
</span>    <span class="n">mcp</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
</code></pre></div></div> <ol> <li>Claude Desktop에 연결하기 만든 서버를 LLM 클라이언트(Claude Desktop)가 인식하도록 설정해야 한다. ~/Library/Application Support/Claude/claude_desktop_config.json 파일을 열고 다음을 추가한다.</li> </ol> <p>JSON</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"mcpServers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"my-weather-server"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"command"</span><span class="p">:</span><span class="w"> </span><span class="s2">"python"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"/absolute/path/to/weather_server.py"</span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>이제 Claude Desktop을 재실행하면, 우측 상단에 🔌 아이콘이 생기고 get_weather 툴이 활성화된 것을 볼 수 있다.</p> <p>채팅창에 <strong>“서울 날씨 어때?”</strong>라고 물어보면, Claude는 내 로컬 파이썬 스크립트를 실행하여 정보를 가져오고 답을 해준다.</p> <ol> <li>핵심 포인트: Tools vs Resources Tools (도구): get_weather(city)처럼 <strong>인자(Argument)</strong>를 받아 연산을 수행하거나 부작용(Side-effect)이 있는 작업에 사용된다. (Function Calling과 유사)</li> </ol> <p>Resources (자원): weather://list처럼 정적인 데이터나 파일 내용을 읽어오는(Read) 용도로 사용된다. 모델에게 “참고 자료”를 던져주는 개념이다.</p> <p>이 두 가지 추상화만 있으면, 파일 시스템, 데이터베이스, API 서버 등 세상의 모든 데이터를 LLM에 연결할 수 있다.</p> <p>다음 포스트에서는 로컬 통신(Stdio)을 넘어, <strong>원격 서버(SSE)</strong>로 배포하고 디버깅하는 심화 엔지니어링 기법을 다룬다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="mcp"/><category term="backend"/><category term="coding"/><category term="tutorial"/><summary type="html"><![CDATA[mcp-python-sdk를 활용하여 SQLite 데이터를 조회하는 커스텀 서버 구현 실습. Tools와 Resources의 개념을 코드로 이해하기.]]></summary></entry><entry><title type="html">[LLM Ops] AI 에이전트의 블랙박스를 여는 기술: H.E.A.R. 감사 로그 전략</title><link href="https://ssolllll.github.io/blog/2025/llm-log/" rel="alternate" type="text/html" title="[LLM Ops] AI 에이전트의 블랙박스를 여는 기술: H.E.A.R. 감사 로그 전략"/><published>2025-12-21T10:00:00+00:00</published><updated>2025-12-21T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/llm-log</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/llm-log/"><![CDATA[<p>LLM이 단순한 챗봇(Chatbot)을 넘어 도구를 사용하고 의사결정을 내리는 <strong>에이전트(Agent)</strong> 로 진화하면서, 엔지니어링의 난이도는 ‘생성’에서 ‘통제’와 ‘추적’으로 이동하고 있다.</p> <p>에이전트가 <code class="language-plaintext highlighter-rouge">계획(Plan) -&gt; 행동(Action) -&gt; 도구 호출(Tool Use) -&gt; 결과 해석(Observation)</code>의 다단계 과정을 자율적으로 수행할 때, 기존의 “Request-Response” 로그만으로는 사고 발생 시 원인을 파악하기 불가능하다.</p> <p>“왜 AI가 대출 승인을 거절했는가?”, “왜 환불 금액을 2만 원으로 책정했는가?”</p> <p>이 질문에 답하기 위해 필요한 <strong>인간이 이해할 수 있는 감사 추적(Audit Trail)</strong> 시스템 구축 전략을 정리한다.</p> <hr/> <h2 id="1-왜-지금-감사-로그인가">1. 왜 지금 ‘감사 로그’인가?</h2> <p>단순 API 호출이 아니라 <strong>의사결정의 연쇄(Chain of Decisions)</strong> 가 일어나기 때문이다.</p> <ol> <li><strong>맥락의 단절:</strong> 사용자의 질문 하나에 에이전트는 내부적으로 수십 번의 API 호출과 추론을 거친다. 결과값만 남기면 중간 과정의 오류(Hallucination)나 잘못된 도구 사용을 찾아낼 수 없다.</li> <li><strong>규제 준수 (Compliance):</strong> 금융권의 ISMS-P, ISO 42001(AI 경영시스템) 등은 AI의 투명성과 설명 가능성을 요구한다.</li> <li><strong>사고 재현 및 디버깅:</strong> 동일한 입력에도 다른 결과를 낼 수 있는 LLM의 비결정성(Non-determinism)을 통제하려면, 당시의 환경과 파라미터 스냅샷이 필수적이다.</li> </ol> <hr/> <h2 id="2-핵심-원칙-hear">2. 핵심 원칙: H.E.A.R.</h2> <p>우리는 기술적인 로그와 비즈니스적인 설명을 분리하되 연결해야 한다. 이를 위한 4가지 원칙을 제안한다.</p> <ul> <li><strong>Human-Readable (가독성):</strong> JSON 덤프와 별개로, 에이전트가 스스로 자신의 행동을 3~5줄로 요약한 ‘설명 노트’를 남겨야 한다. 사람이 로그를 해석하는 시간을 획기적으로 줄여준다.</li> <li><strong>Event-Sourced (이벤트 소싱):</strong> 모든 의사결정과 행동을 불변(Immutable)의 이벤트 스트림으로 저장한다. 이는 나중에 시점별 상태를 재구성하는 기반이 된다.</li> <li><strong>Attribution (책임 소재):</strong> 이 결정이 ‘어떤 프롬프트 버전’, ‘어떤 모델’, ‘어떤 데이터 출처’에서 나왔는지 명확히 분리하여 기록한다.</li> <li><strong>Reproducibility (재현성):</strong> 문제가 된 상황을 똑같이 다시 실행해 볼 수 있도록 환경 변수와 외부 API 응답값까지 스냅샷을 뜬다.</li> </ul> <hr/> <h2 id="3-최소-구현-아키텍처">3. 최소 구현 아키텍처</h2> <h3 id="a-행동-단위-action-unit">A. 행동 단위 (Action Unit)</h3> <p>에이전트의 생각과 행동을 가장 작은 단위로 쪼개어 표준 스키마로 기록한다. <code class="language-plaintext highlighter-rouge">trace_id</code>를 통해 전체 실행 흐름을 트리(Tree) 구조로 엮는다.</p> <ul> <li><strong>Trace Context:</strong> <code class="language-plaintext highlighter-rouge">trace_id</code>, <code class="language-plaintext highlighter-rouge">parent_id</code></li> <li><strong>Thinking:</strong> <code class="language-plaintext highlighter-rouge">intent</code>(사용자 의도), <code class="language-plaintext highlighter-rouge">plan_step</code>(현재 계획 단계)</li> <li><strong>Action:</strong> <code class="language-plaintext highlighter-rouge">tool_name</code>, <code class="language-plaintext highlighter-rouge">args_hash</code>, <code class="language-plaintext highlighter-rouge">input_redacted</code>(마스킹된 입력)</li> <li><strong>Result:</strong> <code class="language-plaintext highlighter-rouge">result_digest</code>(결과 요약 해시), <code class="language-plaintext highlighter-rouge">cost</code>(토큰 비용), <code class="language-plaintext highlighter-rouge">latency</code></li> </ul> <h3 id="b-서술-로그-narrative-log">B. 서술 로그 (Narrative Log)</h3> <p>기계적인 로그 외에, 에이전트에게 <strong>“지금 뭘 했고 왜 했는지”</strong>를 자연어로 남기게 한다.</p> <blockquote> <p><em>“영수증에서 항목을 추출하고(증거 A) 수업료는 교육실비로 분류함. 미술/피아노 등 세부 실기료는 제외 규정에 따라 필터링. 최종 한도 계산 결과 2,500,000원 잔여.”</em></p> </blockquote> <h3 id="c-저장-및-뷰-계층">C. 저장 및 뷰 계층</h3> <ul> <li><strong>Storage:</strong> Kafka/Redpanda 같은 Append-only 로그로 수집 후, S3 같은 객체 스토리지에 장기 보관한다. (WORM 옵션 적용 시 위변조 방지 가능)</li> <li><strong>Viewer:</strong> 단순 텍스트 뷰어가 아닌, 타임라인(Gantt Chart)이나 의사결정 트리 형태로 시각화하여 “어디서 병목이 걸렸는지”, “어디서 리스크 플래그가 떴는지” 한눈에 파악한다.</li> </ul> <hr/> <h2 id="4-스키마-예시-json">4. 스키마 예시 (JSON)</h2> <p>실제 프로덕션 환경에서 사용 가능한 로그 스키마의 예시다.</p> <p>```json { “trace_id”: “7f8a9d12-…”, “parent_id”: “3b2c1e…”, “timestamp”: “2025-12-21T14:30:00Z”, “actor”: { “type”: “agent”, “name”: “tuition-bot”, “model”: “gpt-5-mini:2025-10” // 모델 버전 명시 }, “intent”: “학자금 영수증 항목 분류 및 한도 체크”, “plan_step”: “extract -&gt; classify -&gt; limit-calc”,</p> <p>“tool_call”: { “name”: “ocr.extract”, “args_redacted”: “{‘file_id’: ‘***’, ‘user_id’: ‘user_123’}”, // 민감정보 마스킹 “result_digest”: “sha256:e3b0c44298fc…”, “latency_ms”: 412, “cost”: { “tokens_in”: 1234, “tokens_out”: 256 } },</p> <p>“policy_flags”: [“PII_DETECTED”, “FIN_DATA”], // 리스크 플래그 “evidence”: [ {“type”: “sample”, “path”: “s3://logs/evidence/resp.snip.json”} ],</p> <p>“narrative”: “영수증 OCR 추출 완료. 수업료 항목을 식별하여 교육실비로 분류함. 미술 실기료는 규정에 의거하여 지급 대상에서 제외 처리.” }</p>]]></content><author><name></name></author><category term="engineering"/><category term="llm"/><category term="ai-agent"/><category term="mlops"/><category term="observability"/><category term="security"/><category term="compliance"/><summary type="html"><![CDATA[수백 건의 자율 실행을 수행하는 AI 에이전트. 단순 로그를 넘어 의도(Intent)와 맥락(Context)을 추적하는 감사 시스템 구축 가이드.]]></summary></entry><entry><title type="html">[Transformers] 파이토치 루프 탈출: Trainer API 200% 활용 가이드</title><link href="https://ssolllll.github.io/blog/2025/transformers-api/" rel="alternate" type="text/html" title="[Transformers] 파이토치 루프 탈출: Trainer API 200% 활용 가이드"/><published>2025-12-13T10:00:00+00:00</published><updated>2025-12-13T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/transformers-api</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/transformers-api/"><![CDATA[<p>딥러닝 모델링을 처음 배울 때는 <code class="language-plaintext highlighter-rouge">for epoch in epochs:</code> 로 시작하는 PyTorch의 Raw Loop를 직접 짜는 것이 공부에 도움이 된다. 하지만 <strong>상용 수준의 LLM 학습 파이프라인</strong>을 구축할 때 이 방식은 ‘기술 부채(Technical Debt)’가 되기 쉽다.</p> <p>Mixed Precision(fp16), Gradient Accumulation, Multi-GPU 분산 학습(DDP), Logging, Checkpointing 기능을 매번 직접 구현하고 디버깅하는 것은 비효율적이다.</p> <p>이번 포스트에서는 Hugging Face의 <code class="language-plaintext highlighter-rouge">Trainer</code> API를 단순한 편의 도구가 아닌, <strong>견고한 학습 파이프라인의 표준 규격</strong>으로 활용하는 엔지니어링 팁을 공유한다.</p> <hr/> <h2 id="1-trainer-boilerplate-code-제거와-표준화">1. Trainer: Boilerplate Code 제거와 표준화</h2> <p><code class="language-plaintext highlighter-rouge">Trainer</code> 클래스는 학습에 필요한 모든 Best Practice가 집약되어 있다.</p> <h3 id="-bad-practice-raw-pytorch-loop">🔴 Bad Practice: Raw PyTorch Loop</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 매 프로젝트마다 반복되는 지루한 코드...
</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nc">GradScaler</span><span class="p">()</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
    <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># fp16 처리
</span>    
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># 그라디언트 누적 처리
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
    
    <span class="c1"># 로깅, 체크포인트 저장, 검증 로직 등 수백 줄 추가 필요
</span></code></pre></div></div> <h3 id="-best-practice-trainingarguments-활용">🟢 Best Practice: TrainingArguments 활용</h3> <p><code class="language-plaintext highlighter-rouge">TrainingArguments</code>에 설정값만 넘기면 복잡한 기능들이 내부적으로 최적화되어 실행된다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="n">args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./results</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="c1"># 배치 크기 16 효과
</span>    <span class="n">fp16</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                     <span class="c1"># Mixed Precision 자동 적용
</span>    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="sh">"</span><span class="s">wandb</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># WandB, MLflow 자동 연동
</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="c1"># ...
</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p><strong>Engineering Note:</strong> 팀 내에서 <code class="language-plaintext highlighter-rouge">TrainingArguments</code> 설정을 공유하면, 누가 학습 코드를 짜더라도 동일한 로깅 포맷과 저장 구조를 유지할 수 있어 협업 효율이 극대화된다.</p> </blockquote> <hr/> <h2 id="2-callback-학습-중간에-개입하기-hooking">2. Callback: 학습 중간에 개입하기 (Hooking)</h2> <p><code class="language-plaintext highlighter-rouge">Trainer</code>를 쓰면 내부 로직을 수정하기 어렵다고 생각하기 쉽다. 하지만 <code class="language-plaintext highlighter-rouge">TrainerCallback</code>을 사용하면 <strong>학습의 특정 시점(Start, Step End, Epoch End 등)</strong>에 원하는 코드를 주입(Hook)할 수 있다.</p> <p>LLM 학습 시 가장 유용한 것은 <strong>“Loss만 보지 말고, 실제 텍스트가 잘 생성되는지 확인하는 것”</strong>이다.</p> <h3 id="-best-practice-실시간-생성-평가-callback">🟢 Best Practice: 실시간 생성 평가 Callback</h3> <p>학습 도중(예: 매 500 step 마다) 모델이 멍청한 소리를 하고 있진 않은지 샘플을 생성해서 로그에 찍어보는 콜백이다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">TrainerCallback</span>

<span class="k">class</span> <span class="nc">GenerationCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompt_text</span><span class="o">=</span><span class="sh">"</span><span class="s">User: 안녕?</span><span class="se">\n</span><span class="s">Assistant:</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_text</span>

    <span class="k">def</span> <span class="nf">on_step_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 500 스텝마다 실행
</span>        <span class="k">if</span> <span class="n">state</span><span class="p">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prompt_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1"># 학습 모드(train) -&gt; 평가 모드(eval)로 잠시 전환
</span>            <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
                <span class="n">generated_text</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">[Step </span><span class="si">{</span><span class="n">state</span><span class="p">.</span><span class="n">global_step</span><span class="si">}</span><span class="s">] Sample:</span><span class="se">\n</span><span class="si">{</span><span class="n">generated_text</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
            
            <span class="c1"># 다시 학습 모드로 복귀
</span>            <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># Trainer에 등록
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="p">...,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="nc">GenerationCallback</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)]</span>
<span class="p">)</span>
</code></pre></div></div> <p>이 콜백이 있으면 학습이 산으로 가고 있는지(Overfitting/Collapse)를 Loss 그래프보다 훨씬 직관적으로 파악할 수 있다.</p> <hr/> <h2 id="3-storage-management-디스크-폭발-방지">3. Storage Management: 디스크 폭발 방지</h2> <p>LLM 체크포인트(Checkpoint)는 개당 수십 GB에 달한다. 기본 설정으로 두면 매 <code class="language-plaintext highlighter-rouge">save_steps</code> 마다 저장이 되어 금세 디스크가 가득 찬다.</p> <h3 id="-best-practice-save_total_limit--load_best_model_at_end">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">save_total_limit</code> &amp; <code class="language-plaintext highlighter-rouge">load_best_model_at_end</code></h3> <p>가장 성능이 좋은 모델 하나와, 혹시 모를 중단을 대비한 최신 모델 하나만 남기는 전략이다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./llm-checkpoints</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    
    <span class="c1"># 핵심 옵션:
</span>    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>          <span class="c1"># 가장 최근 2개만 남기고 자동 삭제
</span>    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 학습 종료 시 validation loss가 가장 낮은 모델 로드
</span>    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>이 설정을 통해 디스크 용량을 효율적으로 관리하면서도, 학습이 중간에 멈췄을 때 <code class="language-plaintext highlighter-rouge">trainer.train(resume_from_checkpoint=True)</code>를 통해 언제든 재개할 수 있는 안전장치를 마련할 수 있다.</p> <hr/> <h2 id="결론">결론</h2> <p><code class="language-plaintext highlighter-rouge">Trainer</code> API를 잘 쓴다는 것은 단순히 코드를 줄이는 것을 넘어, <strong>실험의 재현성(Reproducibility)</strong>과 <strong>운영의 안정성(Stability)</strong>을 확보한다는 의미다.</p> <ol> <li><strong>Arguments:</strong> 복잡한 학습 설정을 dataclass 하나로 관리.</li> <li><strong>Callback:</strong> 학습 과정에 유연하게 개입하여 Custom Logic 실행.</li> <li><strong>Checkpointing:</strong> 디스크 용량 관리 자동화.</li> </ol> <p>이제 모델을 학습시키는 파이프라인까지 구축했다. 마지막 단계는 학습된 거대 모델을 더 작고 빠르게 만드는 <strong>“5. Optimization: 양자화(Quantization)와 효율화 기법”</strong>이다. 다음 포스트에서 시리즈를 마무리해보자.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="training"/><category term="mlops"/><category term="backend"/><summary type="html"><![CDATA[Raw PyTorch Loop 대신 Trainer를 써야 하는 이유. Custom Callback을 이용한 실시간 생성 평가와 효율적인 체크포인트 관리 전략.]]></summary></entry><entry><title type="html">[Transformers] LLM 경량화와 가속: Quantization부터 Flash Attention까지</title><link href="https://ssolllll.github.io/blog/2025/bitsandbytes/" rel="alternate" type="text/html" title="[Transformers] LLM 경량화와 가속: Quantization부터 Flash Attention까지"/><published>2025-12-12T10:00:00+00:00</published><updated>2025-12-12T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/bitsandbytes</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/bitsandbytes/"><![CDATA[<p>거대 언어 모델(LLM)을 다룰 때 가장 큰 장벽은 ‘비용’이다. 수천만 원짜리 A100 GPU를 마음껏 쓸 수 있다면 좋겠지만, 현실은 제한된 VRAM(24GB, 48GB) 안에서 최대한의 성능을 뽑아내야 한다.</p> <p>다행히 Hugging Face 생태계는 <strong>모델의 크기를 줄이는(Quantization)</strong> 기술과 <strong>연산 속도를 높이는(Acceleration)</strong> 기술을 매우 쉽게 사용할 수 있도록 통합해 두었다.</p> <p>이번 시리즈의 마지막 포스트에서는 엔지니어링 관점에서 반드시 적용해야 할 3가지 최적화 기법을 다룬다.</p> <hr/> <h2 id="1-quantization-4-bit의-마법-feat-bitsandbytes">1. Quantization: 4-bit의 마법 (feat. BitsAndBytes)</h2> <p>양자화(Quantization)는 32비트(float32)나 16비트(float16)로 표현된 가중치를 8비트(int8) 또는 4비트(nf4)로 압축하여 표현하는 기술이다. 놀라운 점은 4비트로 줄여도 모델의 성능 저하가 미미하다는 것이다.</p> <h3 id="-bad-practice-무조건-원본-로드">🔴 Bad Practice: 무조건 원본 로드</h3> <p>70B 모델을 fp16으로 로드하면 약 140GB의 VRAM이 필요하다. 이는 A100(80GB) 2장을 써야 겨우 올라가는 크기다.</p> <h3 id="-best-practice-bitsandbytesconfig-4-bit-loading">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">BitsAndBytesConfig</code> (4-bit Loading)</h3> <p><code class="language-plaintext highlighter-rouge">bitsandbytes</code> 라이브러리를 통해 모델을 4비트로 로드하면 용량이 약 1/4로 줄어든다. 이를 통해 <strong>Llama-2-70B 모델을 48GB GPU(A6000, L40) 한 장</strong>에 올릴 수 있다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="c1"># 4bit 양자화 설정 (QLoRA 학습 시 필수)
</span><span class="n">bnb_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="sh">"</span><span class="s">nf4</span><span class="sh">"</span><span class="p">,</span>       <span class="c1"># 정규분포에 최적화된 Normal Float 4
</span>    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="c1"># 연산은 bf16으로 수행 (속도/성능 보존)
</span>    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># 양자화 상수를 한 번 더 양자화 (메모리 추가 절약)
</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-70b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>이 설정은 파인튜닝 기법인 <strong>QLoRA(Quantized LoRA)</strong>의 기반이 되며, 소비자용 GPU(RTX 3090/4090)에서도 LLM 학습을 가능하게 만든 일등공신이다.</p> <hr/> <h2 id="2-flash-attention-2-메모리-대역폭-병목-해소">2. Flash Attention 2: 메모리 대역폭 병목 해소</h2> <p>Transformer 구조의 핵심인 Attention 메커니즘은 입력 시퀀스 길이($N$)의 제곱($N^2$)에 비례하여 연산량과 메모리 사용량이 증가한다. 문장이 길어질수록 급격히 느려지는 이유다.</p> <p><strong>Flash Attention</strong>은 GPU 메모리(HBM) 접근 횟수를 획기적으로 줄여(IO-Aware), 긴 문맥 처리 속도를 2배 이상 빠르게 만든다.</p> <h3 id="적용-방법">적용 방법</h3> <p>복잡한 구현 없이 <code class="language-plaintext highlighter-rouge">use_flash_attention_2=True</code> 옵션만 켜면 된다. (단, Ampere 아키텍처 이상의 GPU 필요: A100, A10, RTX 30/40 시리즈)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> 
    <span class="n">attn_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">flash_attention_2</span><span class="sh">"</span> <span class="c1"># 핵심 옵션
</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p><strong>Benchmark:</strong> 입력 토큰이 4096 이상일 때, 일반 Attention 대비 <strong>2~3배의 속도 향상</strong>과 메모리 절약 효과를 볼 수 있다. RAG(검색 증강 생성)처럼 긴 문서를 다루는 서비스에서는 필수 옵션이다.</p> </blockquote> <hr/> <h2 id="3-kv-cache-했던-계산-또-하지-않기">3. KV Cache: 했던 계산 또 하지 않기</h2> <p>LLM은 다음 토큰을 예측할 때, 이전의 모든 토큰 정보를 다시 참조한다. 매번 처음부터 다시 계산하는 것은 엄청난 낭비다. <strong>Key-Value (KV) Cache</strong>는 이전 토큰들의 연산 결과(Key, Value)를 메모리에 저장해두고 재사용하는 기술이다.</p> <h3 id="-best-practice-use_cachetrue">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">use_cache=True</code></h3> <p>대부분의 모델에서 기본값은 <code class="language-plaintext highlighter-rouge">True</code>이지만, 학습(Training) 시에는 메모리 절약을 위해 <code class="language-plaintext highlighter-rouge">False</code>로 끄는 경우가 많다. 따라서 <strong>추론(Inference) 단계에서는 반드시 켜져 있는지 확인</strong>해야 한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. 모델 설정에서 확인
</span><span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># 2. generation 호출 시 명시
</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span> 
<span class="p">)</span>
</code></pre></div></div> <p>KV Cache를 끄면 문장이 길어질수록 생성 속도가 선형적으로 느려지지만, 켜면 문장 길이에 상관없이 일정한 속도(O(1) per token)를 유지할 수 있다.</p> <hr/> <h2 id="series-conclusion-llm-엔지니어링의-여정">Series Conclusion: LLM 엔지니어링의 여정</h2> <p>총 5편에 걸쳐 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리를 활용한 LLM 엔지니어링 핵심 기술을 살펴보았다.</p> <ol> <li><strong>Tokenizer:</strong> Rust 기반 가속과 Dynamic Padding으로 입력 파이프라인 최적화.</li> <li><strong>Model Loading:</strong> <code class="language-plaintext highlighter-rouge">device_map</code>과 <code class="language-plaintext highlighter-rouge">fp16</code>으로 OOM 없는 로딩.</li> <li><strong>Inference:</strong> <code class="language-plaintext highlighter-rouge">GenerationConfig</code>와 <code class="language-plaintext highlighter-rouge">Streamer</code>로 상용 수준의 서비스 구현.</li> <li><strong>Training:</strong> <code class="language-plaintext highlighter-rouge">Trainer</code>와 <code class="language-plaintext highlighter-rouge">Callback</code>으로 안정적인 학습 파이프라인 구축.</li> <li><strong>Optimization:</strong> <code class="language-plaintext highlighter-rouge">Quantization</code>과 <code class="language-plaintext highlighter-rouge">Flash Attention</code>으로 극한의 효율성 달성.</li> </ol> <p>이 시리즈가 단순한 코드 복사-붙여넣기를 넘어, <strong>“왜 이렇게 설정해야 하는가?”</strong>에 대한 엔지니어링적 통찰을 제공했기를 바란다. 이제 여러분의 데이터를 모델에 태워볼 차례다. Happy Engineering!</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="quantization"/><category term="optimization"/><category term="cuda"/><summary type="html"><![CDATA[BitsAndBytes를 활용한 4bit 양자화(QLoRA) 원리와 Flash Attention 2 적용법. KV Cache의 중요성까지, 모델 성능을 극한으로 끌어올리는 최적화 기법 총정리.]]></summary></entry><entry><title type="html">[Transformers] LLM 성능의 시작점, Tokenizer 최적화 및 활용 전략</title><link href="https://ssolllll.github.io/blog/2025/transformers-optimization/" rel="alternate" type="text/html" title="[Transformers] LLM 성능의 시작점, Tokenizer 최적화 및 활용 전략"/><published>2025-12-07T18:00:00+00:00</published><updated>2025-12-07T18:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/transformers-optimization</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/transformers-optimization/"><![CDATA[<p>LLM 엔지니어링에서 모델(Model) 아키텍처나 양자화(Quantization)에는 많은 관심을 갖지만, 정작 데이터가 모델로 들어가는 첫 관문인 <strong>Tokenizer</strong>의 효율성은 간과하는 경우가 많다.</p> <p>하지만 수십 기가바이트의 텍스트를 처리하는 Pre-training/Fine-tuning 단계나, 실시간으로 요청을 처리하는 Serving 단계에서 비효율적인 Tokenizer 사용은 전체 파이프라인의 <strong>심각한 병목(Bottleneck)</strong>이 될 수 있다.</p> <p>이번 포스트에서는 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리의 Tokenizer를 엔지니어링 관점에서 100% 활용하는 세 가지 핵심 전략을 다룬다.</p> <hr/> <h2 id="1-fast-tokenizer-rust의-속도를-빌려라">1. Fast Tokenizer: Rust의 속도를 빌려라</h2> <p>Hugging Face의 <code class="language-plaintext highlighter-rouge">transformers</code>는 두 가지 타입의 토크나이저를 제공한다.</p> <ol> <li><strong>Python Tokenizer:</strong> 순수 파이썬 구현체. 느리다.</li> <li><strong>Fast Tokenizer:</strong> Rust로 구현된 <code class="language-plaintext highlighter-rouge">tokenizers</code> 라이브러리 래퍼(Wrapper). 매우 빠르다.</li> </ol> <p>대부분 <code class="language-plaintext highlighter-rouge">AutoTokenizer.from_pretrained()</code>를 호출하면 자동으로 Fast 버전을 가져오지만, 명시적으로 확인하고 사용하는 습관이 중요하다.</p> <h3 id="-성능-차이-벤치마크">⚡ 성능 차이 벤치마크</h3> <p>수백만 건의 데이터를 전처리할 때, 이 속도 차이는 데이터 로딩 시간(Data Loading Time)을 몇 시간에서 몇 분으로 줄여준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The quick brown fox jumps over the lazy dog.</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">1000</span>

<span class="c1"># 1. Slow Tokenizer (Python)
</span><span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">slow_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Slow: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> sec</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. Fast Tokenizer (Rust)
</span><span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">fast_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Fast: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> sec</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 결과 예시:
# Slow: 1.2031 sec
# Fast: 0.1054 sec (약 10배 이상 빠름)
</span></code></pre></div></div> <blockquote> <p><strong>Tip:</strong> <code class="language-plaintext highlighter-rouge">FastTokenizer</code>는 병렬 처리(Parallelism) 기능도 내장하고 있어, <code class="language-plaintext highlighter-rouge">batched=True</code> 옵션과 함께 사용 시 CPU 코어를 최대한 활용한다.</p> </blockquote> <hr/> <h2 id="2-dynamic-padding-불필요한-연산량-줄이기">2. Dynamic Padding: 불필요한 연산량 줄이기</h2> <p>LLM은 배처(Batch) 처리를 위해 입력 시퀀스의 길이를 통일해야 한다. 이때 부족한 부분을 채우는 것이 <strong>Padding</strong>이다.</p> <h3 id="-bad-practice-고정-길이-padding">🔴 Bad Practice: 고정 길이 Padding</h3> <p>모든 데이터를 모델의 <code class="language-plaintext highlighter-rouge">max_length</code>(예: 2048)에 맞춰 패딩하면, “Hello” 같은 짧은 문장도 2043개의 패딩 토큰(<code class="language-plaintext highlighter-rouge">pad_token</code>)을 계산해야 한다. 이는 GPU 메모리와 연산력의 낭비다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 비효율적인 방식: 무조건 max_length까지 늘림
</span><span class="n">tokenized</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">],</span> 
    <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">max_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="c1"># 결과: [Hello, PAD, PAD, ..., PAD] -&gt; 불필요한 연산 발생
</span></code></pre></div></div> <h3 id="-best-practice-dynamic-padding-datacollator">🟢 Best Practice: Dynamic Padding (DataCollator)</h3> <p>배치(Batch) 내에서 <strong>가장 긴 문장의 길이에 맞춰서</strong> 패딩하는 방식이다. 데이터셋 전체가 아닌, 미니 배치 단위로 길이를 맞추므로 연산량이 획기적으로 줄어든다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorWithPadding</span>

<span class="c1"># 데이터셋 단계에서는 패딩하지 않음 (truncation만 적용)
</span><span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># DataLoader가 배치를 만들 때 패딩 수행
</span><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Trainer나 DataLoader에 collator 전달
# trainer = Trainer(..., data_collator=data_collator)
</span></code></pre></div></div> <p>이 방식을 적용하면 학습 속도가 데이터 길이에 따라 <strong>20~30% 이상 향상</strong>될 수 있다.</p> <hr/> <h2 id="3-chat-template-프롬프트-엔지니어링의-표준화">3. Chat Template: 프롬프트 엔지니어링의 표준화</h2> <p>LLM마다 요구하는 프롬프트 포맷이 다르다. (Llama-2의 <code class="language-plaintext highlighter-rouge">[INST]</code>, ChatML의 <code class="language-plaintext highlighter-rouge">&lt;|im_start|&gt;</code>, Alpha의 <code class="language-plaintext highlighter-rouge">Human:</code> 등) 이를 하드코딩으로 문자열을 합치는 방식은 모델 교체 시 코드를 전면 수정해야 하는 리스크가 있다.</p> <h3 id="apply_chat_template-활용"><code class="language-plaintext highlighter-rouge">apply_chat_template</code> 활용</h3> <p><code class="language-plaintext highlighter-rouge">transformers</code> 4.34.0부터 도입된 이 기능은 모델의 <code class="language-plaintext highlighter-rouge">tokenizer_config.json</code>에 정의된 템플릿을 자동으로 적용해준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">RAG 시스템의 장점은?</span><span class="sh">"</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">최신 정보를 반영할 수 있다는 점입니다.</span><span class="sh">"</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">단점은?</span><span class="sh">"</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># 모델에 맞는 포맷으로 자동 변환
</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span> 
    <span class="n">tokenize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># Llama-2인 경우: &lt;s&gt;[INST] RAG 시스템의 장점은? [/INST] 최신 정보를 ... &lt;/s&gt;&lt;s&gt;[INST] 단점은? [/INST]
# Mistral인 경우: ... (해당 모델 포맷 자동 적용)
</span></code></pre></div></div> <p>이 기능을 사용하면 <strong>모델 의존성(Model Dependency)을 코드에서 분리</strong>할 수 있어, 여러 모델을 실험하거나 A/B 테스트를 진행할 때 유지보수성이 극대화된다.</p> <hr/> <h2 id="결론">결론</h2> <p>Tokenizer는 단순히 자연어 처리를 위한 전처리 도구가 아니다.</p> <ul> <li><strong>Rust 기반 Fast Tokenizer</strong>로 CPU 병목을 해소하고,</li> <li><strong>Dynamic Padding</strong>으로 GPU 연산 효율을 높이며,</li> <li><strong>Chat Template</strong>으로 코드의 유연성을 확보하는 것.</li> </ul> <p>이것이 LLM 엔지니어가 갖춰야 할 “입력 파이프라인 최적화”의 기본기다. 다음 포스트에서는 이렇게 처리된 데이터를 거대 모델에 효율적으로 적재하는 <strong>Model Loading 전략</strong>에 대해 다루겠다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="llm"/><category term="optimization"/><category term="nlp"/><summary type="html"><![CDATA[Hugging Face Tokenizer의 Rust 기반 가속, Dynamic Padding을 통한 연산량 절감, 그리고 Chat Template을 활용한 프롬프트 엔지니어링 표준화 가이드.]]></summary></entry><entry><title type="html">[Transformers] LLM 추론 제어: GenerationConfig와 실시간 Streaming 구현</title><link href="https://ssolllll.github.io/blog/2025/generation_config_streaming/" rel="alternate" type="text/html" title="[Transformers] LLM 추론 제어: GenerationConfig와 실시간 Streaming 구현"/><published>2025-12-07T10:00:00+00:00</published><updated>2025-12-07T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/generation_config_streaming</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/generation_config_streaming/"><![CDATA[<p>LLM을 서비스에 도입할 때 사용자들이 가장 먼저 체감하는 성능 지표는 무엇일까? 전체 답변이 완성되는 시간(Total Latency)보다, <strong>첫 번째 글자가 화면에 찍히는 시간(TTFT, Time To First Token)</strong>이다.</p> <p><code class="language-plaintext highlighter-rouge">model.generate()</code> 함수는 기본적으로 모든 생성이 끝날 때까지 블로킹(Blocking)된다. 챗봇 서비스에서 사용자에게 10초 동안 로딩 바만 보여주는 것은 최악의 경험이다.</p> <p>이번 포스트에서는 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리를 사용해 <strong>ChatGPT처럼 글자가 타닥타닥 찍히는 스트리밍</strong>을 구현하는 방법과, 복잡한 생성 하이퍼파라미터를 코드가 아닌 설정 파일로 관리하는 <strong>GenerationConfig</strong> 활용법을 다룬다.</p> <hr/> <h2 id="1-generationconfig-파라미터-하드코딩-멈춰">1. GenerationConfig: 파라미터 하드코딩 멈춰!</h2> <p><code class="language-plaintext highlighter-rouge">temperature</code>, <code class="language-plaintext highlighter-rouge">top_p</code>, <code class="language-plaintext highlighter-rouge">repetition_penalty</code> 등 LLM의 답변 품질을 결정하는 수많은 파라미터들이 있다. 이를 함수 인자에 하드코딩하는 것은 유지보수 측면에서 좋지 않다.</p> <h3 id="-bad-practice-하드코딩">🔴 Bad Practice: 하드코딩</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 코드를 수정해야만 답변 스타일을 바꿀 수 있음
</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span> 
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="-best-practice-generationconfig-객체-사용">🟢 Best Practice: GenerationConfig 객체 사용</h3> <p><code class="language-plaintext highlighter-rouge">GenerationConfig</code>를 사용하면 파라미터를 객체로 관리할 수 있으며, JSON으로 저장하거나 불러오기가 쉬워 <strong>MLOps(실험 관리)</strong> 관점에서 유리하다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">GenerationConfig</span>

<span class="c1"># 1. 설정 생성 및 저장
</span><span class="n">config</span> <span class="o">=</span> <span class="nc">GenerationConfig</span><span class="p">(</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.1</span>
<span class="p">)</span>
<span class="n">config</span><span class="p">.</span><span class="nf">save_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./my_chat_config</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. 로드 및 사용 (코드는 그대로 두고 설정 파일만 바꿔서 배포 가능)
</span><span class="n">loaded_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./my_chat_config</span><span class="sh">"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">loaded_config</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p><strong>Tip:</strong> 모델마다 “창의적인 모드”, “정확한 모드” 등 여러 프리셋을 미리 만들어두고 <code class="language-plaintext highlighter-rouge">config</code>만 교체하여 요청을 처리할 수 있다.</p> </blockquote> <hr/> <h2 id="2-streamer-실시간-토큰-출력-feat-thread">2. Streamer: 실시간 토큰 출력 (Feat. Thread)</h2> <p>사용자 경험(UX)을 혁신적으로 개선하는 스트리밍은 <code class="language-plaintext highlighter-rouge">TextIteratorStreamer</code>를 통해 구현한다. 핵심은 <code class="language-plaintext highlighter-rouge">model.generate()</code>가 메인 스레드를 점유하지 않도록 <strong>별도 스레드(Thread)</strong>에서 실행해야 한다는 점이다.</p> <h3 id="구현-패턴-standard-pattern">구현 패턴 (Standard Pattern)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">TextIteratorStreamer</span>

<span class="c1"># 1. Streamer 준비
# skip_prompt=True: 입력했던 질문은 다시 출력하지 않음
</span><span class="n">streamer</span> <span class="o">=</span> <span class="nc">TextIteratorStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 2. Generate 함수에 전달할 인자 준비
</span><span class="n">generate_kwargs</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="n">loaded_config</span>
<span class="p">)</span>

<span class="c1"># 3. 별도 스레드에서 생성 시작 (Non-blocking)
# thread를 쓰지 않으면 generate가 끝날 때까지 아래 for문이 실행되지 않음
</span><span class="n">thread</span> <span class="o">=</span> <span class="nc">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">generate_kwargs</span><span class="p">)</span>
<span class="n">thread</span><span class="p">.</span><span class="nf">start</span><span class="p">()</span>

<span class="c1"># 4. 메인 스레드에서는 Streamer에서 나오는 토큰을 실시간으로 소비
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Assistant: </span><span class="sh">"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">)</span>
<span class="k">for</span> <span class="n">new_text</span> <span class="ow">in</span> <span class="n">streamer</span><span class="p">:</span>
    <span class="c1"># 여기서 WebSocket이나 Server-Sent Events(SSE)로 클라이언트에 전송
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">new_text</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 스레드 종료 대기
</span><span class="n">thread</span><span class="p">.</span><span class="nf">join</span><span class="p">()</span>
</code></pre></div></div> <p>이 코드는 <code class="language-plaintext highlighter-rouge">FastAPI</code>나 <code class="language-plaintext highlighter-rouge">Flask</code> 같은 백엔드 프레임워크와 결합하여 <strong>SSE(Server-Sent Events)</strong> 엔드포인트를 구축할 때 그대로 사용되는 핵심 로직이다.</p> <hr/> <h2 id="3-stoppingcriteria-생성을-강제로-멈추는-안전장치">3. StoppingCriteria: 생성을 강제로 멈추는 안전장치</h2> <p>가끔 LLM이 문장을 끝맺지 못하고 <code class="language-plaintext highlighter-rouge">\n\nUser: \n\nUser:</code> 처럼 무한 루프에 빠지거나, 특정 단어가 나오면 즉시 생성을 멈춰야 할 때가 있다. <code class="language-plaintext highlighter-rouge">EOS(End of Sentence)</code> 토큰만 믿기에는 불안하다.</p> <p>이때 <code class="language-plaintext highlighter-rouge">StoppingCriteria</code>를 사용한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">StoppingCriteria</span><span class="p">,</span> <span class="n">StoppingCriteriaList</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">StopOnWord</span><span class="p">(</span><span class="n">StoppingCriteria</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">stop_word_id</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stop_word_id</span> <span class="o">=</span> <span class="n">stop_word_id</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 방금 생성된 토큰이 금지어(stop_word)와 같으면 True 반환 -&gt; 생성 중단
</span>        <span class="k">return</span> <span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">stop_word_id</span>

<span class="c1"># 예: "###" 이라는 토큰이 나오면 즉시 멈춤
</span><span class="n">stop_criteria</span> <span class="o">=</span> <span class="nc">StoppingCriteriaList</span><span class="p">([</span><span class="nc">StopOnWord</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">###</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">])])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span> 
    <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stop_criteria</span>
<span class="p">)</span>
</code></pre></div></div> <p>이 기능은 프롬프트 인젝션을 방어하거나, 멀티턴 대화에서 모델이 사용자 역할까지 연기하는 것을 방지하는 데 필수적이다.</p> <hr/> <h2 id="결론">결론</h2> <p>LLM 추론 제어 기술은 단순히 텍스트를 생성하는 것을 넘어, <strong>사용자가 기다릴 수 있는 속도</strong>와 <strong>의도된 대로만 행동하게 하는 통제권</strong>을 확보하는 것이다.</p> <ul> <li><strong>GenerationConfig:</strong> 하이퍼파라미터의 버전 관리와 디커플링.</li> <li><strong>Streamer + Thread:</strong> TTFT를 줄이고 체감 속도를 높이는 UX의 핵심.</li> <li><strong>StoppingCriteria:</strong> 모델의 폭주를 막는 안전장치.</li> </ul> <p>이제 모델을 로드하고(<code class="language-plaintext highlighter-rouge">Model Loading</code>), 입력을 최적화하고(<code class="language-plaintext highlighter-rouge">Tokenizer</code>), 출력을 제어(<code class="language-plaintext highlighter-rouge">Inference Control</code>)하는 방법까지 익혔다. 다음 포스트에서는 이 모델을 우리 데이터에 맞게 튜닝하는 <strong>Training Loop와 Trainer API 활용법</strong>에 대해 다루겠다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="llm"/><category term="streaming"/><category term="inference"/><category term="backend"/><summary type="html"><![CDATA[하드코딩된 파라미터를 GenerationConfig로 분리하여 관리하는 MLOps 노하우와 TextIteratorStreamer를 활용한 ChatGPT 스타일의 실시간 토큰 스트리밍 구현법.]]></summary></entry><entry><title type="html">[Transformers] OOM 없는 세상: 거대 모델 메모리 적재 및 Offloading 전략</title><link href="https://ssolllll.github.io/blog/2025/offloading/" rel="alternate" type="text/html" title="[Transformers] OOM 없는 세상: 거대 모델 메모리 적재 및 Offloading 전략"/><published>2025-12-07T10:00:00+00:00</published><updated>2025-12-07T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/offloading</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/offloading/"><![CDATA[<p>LLM을 다루는 엔지니어에게 가장 두려운 에러 메시지는 단연 <code class="language-plaintext highlighter-rouge">CUDA out of memory</code>일 것이다. 모델의 파라미터 수는 7B, 13B를 넘어 70B, 405B로 커져가는데, 우리가 가진 GPU VRAM은 언제나 한정적이다.</p> <p>단순히 <code class="language-plaintext highlighter-rouge">.to("cuda")</code>를 호출하는 방식은 딥러닝 초기(BERT 시절)의 유산이다. 수십 기가바이트에 달하는 LLM을 로딩할 때는 <strong>Model Parallelism(모델 병렬화)</strong>과 <strong>Offloading(오프로딩)</strong> 기술이 필수적이다.</p> <p>이번 포스트에서는 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리가 제공하는 스마트한 모델 로딩 기법들을 정리한다.</p> <hr/> <h2 id="1-precision정밀도-타협-fp32에서-fp16bf16으로">1. Precision(정밀도) 타협: fp32에서 fp16/bf16으로</h2> <p>모델의 가중치(Weight)를 어떤 데이터 타입으로 불러오느냐에 따라 메모리 사용량은 즉시 <strong>50%</strong> 절감된다.</p> <ul> <li><strong>float32 (Full Precision):</strong> 파라미터당 4 bytes. (기본값)</li> <li><strong>float16 / bfloat16 (Half Precision):</strong> 파라미터당 2 bytes.</li> </ul> <h3 id="-bad-practice-기본-로딩">🔴 Bad Practice: 기본 로딩</h3> <p>아무 옵션 없이 모델을 로드하면 기본적으로 <code class="language-plaintext highlighter-rouge">float32</code>로 로드되는 경우가 많다. 7B 모델 기준 약 <strong>28GB</strong>의 VRAM이 필요하다. (7B * 4bytes)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># VRAM 낭비의 주범
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-hf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span> 
</code></pre></div></div> <h3 id="-best-practice-torch_dtype-명시">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">torch_dtype</code> 명시</h3> <p><code class="language-plaintext highlighter-rouge">float16</code>을 사용하면 7B 모델을 약 <strong>14GB</strong>에 로드할 수 있다. Ampere 아키텍처(A100, A10, RTX 30/40 시리즈) 이상을 사용한다면 <code class="language-plaintext highlighter-rouge">bfloat16</code>을 사용하는 것이 학습 안정성 면에서 더 유리하다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span>  <span class="c1"># 또는 torch.float16
</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="2-model-offloading-gpu가-넘치면-cpu로">2. Model Offloading: GPU가 넘치면 CPU로</h2> <p>모델이 GPU 메모리보다 클 때, 과거에는 “실행 불가”였다. 하지만 이제는 <strong>Offloading</strong> 기술을 통해 모델의 일부를 CPU RAM이나 디스크로 내려보내고, 추론 시 필요한 레이어만 GPU로 불러와 연산할 수 있다.</p> <h3 id="-best-practice-device_mapauto">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">device_map="auto"</code></h3> <p><code class="language-plaintext highlighter-rouge">accelerate</code> 라이브러리를 기반으로 작동하는 이 옵션은 모델의 레이어를 분석하여 최적의 배치 전략을 자동으로 짠다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-70b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span>  <span class="c1"># 핵심 옵션
</span><span class="p">)</span>

<span class="c1"># 현재 모델이 어떻게 분산되었는지 확인
</span><span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">hf_device_map</span><span class="p">)</span>
<span class="c1"># 출력 예시:
# {
#   'model.embed_tokens': 0,        # GPU 0번
#   'model.layers.0': 0,
#   ...
#   'model.layers.40': 'cpu',       # GPU 꽉 차서 CPU로 이동
#   'lm_head': 'disk'               # RAM도 부족해서 디스크로 이동
# }
</span></code></pre></div></div> <blockquote> <p><strong>Tip:</strong> 추론 속도는 당연히 느려지지만(PCIe 대역폭 병목), <strong>“OOM으로 죽는 것보다 느리더라도 돌아가는 것이 낫다”</strong>는 엔지니어링 원칙 하에 매우 유용한 옵션이다.</p> </blockquote> <hr/> <h2 id="3-max_memory-추론을-위한-여유-공간-확보">3. <code class="language-plaintext highlighter-rouge">max_memory</code>: 추론을 위한 여유 공간 확보</h2> <p><code class="language-plaintext highlighter-rouge">device_map="auto"</code>는 가능한 한 GPU 메모리를 꽉 채워서 모델을 적재하려고 한다. 하지만 LLM 추론 시에는 모델 가중치 외에도 <strong>KV Cache(Context 저장소)</strong>와 <strong>Activation</strong>을 위한 동적 메모리 공간이 필요하다.</p> <p>VRAM을 100% 모델 적재에 써버리면, 막상 추론을 시작하자마자 OOM이 발생한다. 이를 방지하기 위해 GPU 메모리 사용 한도를 제한해야 한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GPU 0번은 20GB까지만 쓰고, 나머지는 CPU(RAM) 100GB를 쓰겠다.
</span><span class="n">max_memory_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">20GiB</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">100GiB</span><span class="sh">"</span><span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-13b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_memory</span><span class="o">=</span><span class="n">max_memory_mapping</span>
<span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="4-initialization-최적화-low_cpu_mem_usage">4. Initialization 최적화: <code class="language-plaintext highlighter-rouge">low_cpu_mem_usage</code></h2> <p>PyTorch의 전통적인 모델 로딩 방식은 <strong>1) 모델 구조(껍데기)를 RAM에 생성</strong>하고 <strong>2) 가중치 파일을 읽어서 덮어쓰는</strong> 방식이다. 이 과정에서 모델 크기의 2배에 달하는 시스템 RAM이 순간적으로 필요하다. (RAM 폭발의 원인)</p> <p>Hugging Face는 <strong>Meta Device</strong> 기술을 활용하여, 빈 껍데기를 생성하지 않고 가중치를 로드하는 <code class="language-plaintext highlighter-rouge">low_cpu_mem_usage=True</code> 옵션을 제공한다. (<code class="language-plaintext highlighter-rouge">device_map="auto"</code> 사용 시 기본값으로 켜짐)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 명시적으로 사용할 때
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">bigscience/bloom</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="결론">결론</h2> <p>거대 모델을 로딩하는 것은 “어떻게 하면 GPU VRAM이라는 비싼 부동산을 알뜰하게 쓸 것인가”에 대한 문제다.</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">torch_dtype</code></strong>: fp16/bf16으로 용량을 절반으로 줄여라.</li> <li><strong><code class="language-plaintext highlighter-rouge">device_map="auto"</code></strong>: GPU 용량이 부족하면 CPU와 Disk를 활용해라.</li> <li><strong><code class="language-plaintext highlighter-rouge">max_memory</code></strong>: 추론 시 생성될 토큰을 위해 VRAM에 여유 공간(Buffer)을 남겨둬라.</li> </ol> <p>이 세 가지만 기억해도 로컬 환경이나 저사양 서버에서 LLM을 실험할 때 겪는 대부분의 메모리 문제는 해결된다. 다음 포스트에서는 이렇게 로드한 모델을 사용해 <strong>실제 텍스트를 생성할 때(Inference)의 제어 기법</strong>에 대해 알아보겠다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="llm"/><category term="gpu"/><category term="memory"/><category term="optimization"/><summary type="html"><![CDATA[AutoModel 로딩 시 발생하는 메모리 부족(OOM) 해결법. device_map을 활용한 Offloading 원리와 torch_dtype 설정을 통한 VRAM 최적화 가이드.]]></summary></entry><entry><title type="html">[Python] collections 모듈 심층 분석</title><link href="https://ssolllll.github.io/blog/2025/python-package-collections/" rel="alternate" type="text/html" title="[Python] collections 모듈 심층 분석"/><published>2025-12-05T10:00:00+00:00</published><updated>2025-12-05T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/python-package-collections</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/python-package-collections/"><![CDATA[<p>LLM Model 서비스를 개발하다 보면 모델 자체의 추론 속도뿐만 아니라, 전후처리 파이프라인의 효율성이 전체 시스템 성능을 좌우하는 경우를 자주 마주합니다.ㄴ</p> <p>특히 Python의 기본 자료구조(<code class="language-plaintext highlighter-rouge">list</code>, <code class="language-plaintext highlighter-rouge">dict</code>)는 범용적이지만, <strong>실시간 스트리밍 처리가 필요한 Chat Completions API</strong>나 <strong>수십만 개의 Vector Search 결과를 다루는 RAG(Retrieval-Augmented Generation)</strong> 과정에서는 미세한 성능 저하와 불필요한 메모리 점유의 원인이 됩니다.</p> <p><code class="language-plaintext highlighter-rouge">collections</code> 모듈을 단순한 편의 도구가 아닌, <strong>엔지니어링 관점의 최적화 도구</strong>로 재해석하고 LLM 실무 적용 사례를 기록합니다.</p> <hr/> <h2 id="1-deque-context-window-관리와-streaming-buffer">1. deque: Context Window 관리와 Streaming Buffer</h2> <p>LLM은 입력 가능한 토큰 길이에 물리적 한계가 있다. 챗봇 구현 시 가장 오래된 대화를 삭제하고 새 대화를 넣는 <strong>Sliding Window</strong> 전략이 필수적이다. 또한, SSE(Server-Sent Events) 스트리밍 시 문장 단위 처리를 위한 버퍼링에도 큐 구조가 필요합니다.</p> <h3 id="-bad-practice-list-사용">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">list</code> 사용</h3> <p>Python의 <code class="language-plaintext highlighter-rouge">list</code>는 동적 배열(Dynamic Array)이다. <code class="language-plaintext highlighter-rouge">pop(0)</code> 연산은 첫 번째 요소를 제거한 뒤, 나머지 <strong>모든 요소의 인덱스를 한 칸씩 당겨오는(Shift)</strong> 작업을 수행합니다.</p> <ul> <li><strong>시간 복잡도:</strong> $O(N)$</li> <li><strong>문제점:</strong> 대화 턴(Turn)이 길어질수록, 동시 접속자가 늘어날수록 CPU 사이클을 낭비하며, GC(Garbage Collection) 압력을 높인다.</li> </ul> <h3 id="-best-practice-deque-circular-buffer">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">deque</code> (Circular Buffer)</h3> <p><code class="language-plaintext highlighter-rouge">collections.deque</code>는 Linked List 기반의 양방향 큐다. 양 끝단에서의 삽입/삭제가 메모리 이동 없이 포인터 변경만으로 이루어진다.</p> <p><img src="/assets/img/deque_vs_list_memory.png" alt="Deque vs List Memory Structure"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">class</span> <span class="nc">ContextManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">max_turns</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># maxlen을 지정하면 꽉 찼을 때 append 시 
</span>        <span class="c1"># 자동으로 반대편 데이터가 O(1)로 삭제됨 (Memory Allocation 발생 안 함)
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_turns</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_dialogue</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">assistant</span><span class="p">):</span>
        <span class="c1"># 튜플로 저장하여 불변성(Immutability) 보장 및 메모리 절약
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="n">user</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="n">assistant</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">get_prompt_context</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># LLM API 요청 직전에만 list로 변환 (변환 비용은 무시할 수준)
</span>        <span class="k">return</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">}</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">]</span>
</code></pre></div></div> <blockquote> <p><strong>Key Engineering Point:</strong> <code class="language-plaintext highlighter-rouge">maxlen</code>을 설정한 <code class="language-plaintext highlighter-rouge">deque</code>는 링 버퍼(Ring Buffer)처럼 동작하여, 오래된 데이터를 삭제하는 코드를 별도로 작성할 필요가 없어 코드가 간결해지고 실수가 줄어든다.</p> </blockquote> <hr/> <h2 id="2-namedtuple-대규모-vector-search-결과의-메모리-경량화">2. namedtuple: 대규모 Vector Search 결과의 메모리 경량화</h2> <p>RAG(검색 증강 생성) 시스템에서는 Vector DB로부터 수천 개의 청크(Chunk)를 검색하고, 이를 Reranking 하는 과정을 거친다. 이때 각 검색 결과를 <code class="language-plaintext highlighter-rouge">dict</code>로 관리하는 것은 메모리 비효율적이다.</p> <h3 id="-bad-practice-dict-리스트-사용">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">dict</code> 리스트 사용</h3> <p>Python의 <code class="language-plaintext highlighter-rouge">dict</code>는 해시 테이블 구조로, 데이터를 저장할 때 Key 값 자체와 해시 충돌 방지를 위한 추가 공간을 점유한다. 객체 하나당 오버헤드가 크다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 일반적인 딕셔너리 구조
</span><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">doc_1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.89</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="p">}</span> 
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="p">]</span>
<span class="c1"># 수만 건의 검색 결과 처리 시 메모리 사용량 급증
</span></code></pre></div></div> <h3 id="-best-practice-namedtuple-활용">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">namedtuple</code> 활용</h3> <p><code class="language-plaintext highlighter-rouge">collections.namedtuple</code>은 내부적으로 튜플(Tuple) 구조를 사용하되, 필드명으로 접근할 수 있게 해준다. <code class="language-plaintext highlighter-rouge">__dict__</code> 속성을 가지지 않아 메모리 사용량이 현저히 적다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># 검색 결과 구조체 정의
</span><span class="n">SearchResult</span> <span class="o">=</span> <span class="nf">namedtuple</span><span class="p">(</span><span class="sh">"</span><span class="s">SearchResult</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># 메모리 사용량 비교 실험
</span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">}</span>
<span class="n">n</span> <span class="o">=</span> <span class="nc">SearchResult</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dict Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="nf">getsizeof</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span>       <span class="c1"># 예: 232 bytes
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NamedTuple Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="nf">getsizeof</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 예: 72 bytes
# -&gt; 약 3배 이상의 메모리 절약 효과
</span></code></pre></div></div> <p><strong>적용 시나리오:</strong></p> <ul> <li>Vector DB에서 가져온 1차 후보군(Top-K 100~1000개)을 메모리에 올릴 때.</li> <li>Reranker 모델에 배치(Batch)로 데이터를 넘기기 전 전처리 과정.</li> </ul> <hr/> <h2 id="3-defaultdict-문서-청크chunk-그룹핑-가속화">3. defaultdict: 문서 청크(Chunk) 그룹핑 가속화</h2> <p>긴 문서를 처리할 때, 여러 개의 청크로 나뉘어 처리된 결과를 다시 원본 문서 ID 기준으로 합쳐야 하는 경우가 있다(Map-Reduce 패턴).</p> <h3 id="-bad-practice-if-key-in-dict-체크">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">if key in dict</code> 체크</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
    <span class="n">doc_id</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">doc_id</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">doc_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span><span class="p">[</span><span class="n">doc_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results</span><span class="p">[</span><span class="n">doc_id</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">summary</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <p>이 방식은 매 반복마다 Key 존재 여부를 해싱하여 검사하므로 불필요한 연산이 포함된다.</p> <h3 id="-best-practice-defaultdict">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">defaultdict</code></h3> <p><code class="language-plaintext highlighter-rouge">collections.defaultdict</code>는 Key가 없을 때의 초기화 로직을 C 레벨에서 처리하므로, Python 레벨의 분기문(Branching)을 제거하여 루프 속도를 높인다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="c1"># 원본 문서 ID별로 요약본을 모으는 파이프라인
</span><span class="n">aggregated_data</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
    <span class="c1"># Key 검사 로직 삭제 -&gt; 코드 가독성 향상 및 속도 최적화
</span>    <span class="n">aggregated_data</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">doc_id</span><span class="sh">'</span><span class="p">]].</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">summary</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="4-chainmap-llm-하이퍼파라미터-계층-관리">4. ChainMap: LLM 하이퍼파라미터 계층 관리</h2> <p>LLM 추론 시 <code class="language-plaintext highlighter-rouge">Temperature</code>, <code class="language-plaintext highlighter-rouge">Top_p</code>, <code class="language-plaintext highlighter-rouge">Max_tokens</code> 등의 설정은 <strong>[기본 설정] -&gt; [유저 설정] -&gt; [프롬프트별 임시 설정]</strong> 순으로 우선순위를 가진다. 이를 <code class="language-plaintext highlighter-rouge">dict.update()</code>로 매번 새로운 딕셔너리를 생성하여 병합하는 것은 비효율적이다.</p> <h3 id="-best-practice-chainmap">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">ChainMap</code></h3> <p><code class="language-plaintext highlighter-rouge">collections.ChainMap</code>은 여러 딕셔너리를 실제로 병합(Copy)하지 않고, 연결된 리스트처럼 논리적으로만 병합하여 보여준다. 조회 시 앞쪽 딕셔너리부터 탐색한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">ChainMap</span>

<span class="n">default_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">}</span>
<span class="n">user_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span>  <span class="c1"># 유저가 온도를 낮춤
</span><span class="n">request_override</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1024</span><span class="p">}</span> <span class="c1"># 이번 요청만 길게
</span>
<span class="c1"># 새로운 딕셔너리 생성 없이 논리적 뷰만 생성 (Zero-copy)
</span><span class="n">final_config</span> <span class="o">=</span> <span class="nc">ChainMap</span><span class="p">(</span><span class="n">request_override</span><span class="p">,</span> <span class="n">user_config</span><span class="p">,</span> <span class="n">default_config</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">])</span> <span class="c1"># 0.5 (user_config 적용)
</span><span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">max_tokens</span><span class="sh">'</span><span class="p">])</span>  <span class="c1"># 1024 (request_override 적용)
</span><span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">top_p</span><span class="sh">'</span><span class="p">])</span>       <span class="c1"># 0.9 (default_config 적용)
</span></code></pre></div></div> <p>이 방식은 요청마다 거대한 설정 객체를 복사할 필요가 없어 <strong>High-Throughput API 서버</strong>에서 GC 오버헤드를 줄이는 데 기여한다.</p> <hr/> <h2 id="결론-micro-optimization이-모여-시스템-안정성을-만든다">결론: Micro-Optimization이 모여 시스템 안정성을 만든다</h2> <p>Python은 느리다는 편견이 있지만, 내장된 <code class="language-plaintext highlighter-rouge">collections</code> 모듈을 적재적소에 활용하면 C로 구현된 내부 최적화의 혜택을 그대로 누릴 수 있다.</p> <ul> <li><strong>Queueing/History:</strong> <code class="language-plaintext highlighter-rouge">deque</code></li> <li><strong>Data Object:</strong> <code class="language-plaintext highlighter-rouge">namedtuple</code></li> <li><strong>Grouping:</strong> <code class="language-plaintext highlighter-rouge">defaultdict</code></li> <li><strong>Config Merging:</strong> <code class="language-plaintext highlighter-rouge">ChainMap</code></li> </ul> <p>LLM 서비스는 텍스트라는 비정형 데이터를 대량으로 다루는 만큼, 이러한 자료구조의 선택이 <strong>Latency(지연 시간)</strong>와 <strong>Memory Footprint(메모리 사용량)</strong>에 미치는 영향이 생각보다 크다. 화려한 모델 튜닝 이전에, 견고한 데이터 파이프라인 구축이 선행되어야 함을 잊지 말자.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="llm"/><category term="optimization"/><category term="backend"/><summary type="html"><![CDATA[단순한 문법 소개가 아닌, 나에게 있어 필요한 collections 모듈 분석.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://ssolllll.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/plotly</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry></feed>