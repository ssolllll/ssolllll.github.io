<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ssolllll.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ssolllll.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-13T07:48:24+00:00</updated><id>https://ssolllll.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[Transformers] LLM 경량화와 가속: Quantization부터 Flash Attention까지</title><link href="https://ssolllll.github.io/blog/2025/bitsandbytes/" rel="alternate" type="text/html" title="[Transformers] LLM 경량화와 가속: Quantization부터 Flash Attention까지"/><published>2025-12-12T10:00:00+00:00</published><updated>2025-12-12T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/bitsandbytes</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/bitsandbytes/"><![CDATA[<p>거대 언어 모델(LLM)을 다룰 때 가장 큰 장벽은 ‘비용’이다. 수천만 원짜리 A100 GPU를 마음껏 쓸 수 있다면 좋겠지만, 현실은 제한된 VRAM(24GB, 48GB) 안에서 최대한의 성능을 뽑아내야 한다.</p> <p>다행히 Hugging Face 생태계는 <strong>모델의 크기를 줄이는(Quantization)</strong> 기술과 <strong>연산 속도를 높이는(Acceleration)</strong> 기술을 매우 쉽게 사용할 수 있도록 통합해 두었다.</p> <p>이번 시리즈의 마지막 포스트에서는 엔지니어링 관점에서 반드시 적용해야 할 3가지 최적화 기법을 다룬다.</p> <hr/> <h2 id="1-quantization-4-bit의-마법-feat-bitsandbytes">1. Quantization: 4-bit의 마법 (feat. BitsAndBytes)</h2> <p>양자화(Quantization)는 32비트(float32)나 16비트(float16)로 표현된 가중치를 8비트(int8) 또는 4비트(nf4)로 압축하여 표현하는 기술이다. 놀라운 점은 4비트로 줄여도 모델의 성능 저하가 미미하다는 것이다.</p> <h3 id="-bad-practice-무조건-원본-로드">🔴 Bad Practice: 무조건 원본 로드</h3> <p>70B 모델을 fp16으로 로드하면 약 140GB의 VRAM이 필요하다. 이는 A100(80GB) 2장을 써야 겨우 올라가는 크기다.</p> <h3 id="-best-practice-bitsandbytesconfig-4-bit-loading">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">BitsAndBytesConfig</code> (4-bit Loading)</h3> <p><code class="language-plaintext highlighter-rouge">bitsandbytes</code> 라이브러리를 통해 모델을 4비트로 로드하면 용량이 약 1/4로 줄어든다. 이를 통해 <strong>Llama-2-70B 모델을 48GB GPU(A6000, L40) 한 장</strong>에 올릴 수 있다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="c1"># 4bit 양자화 설정 (QLoRA 학습 시 필수)
</span><span class="n">bnb_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="sh">"</span><span class="s">nf4</span><span class="sh">"</span><span class="p">,</span>       <span class="c1"># 정규분포에 최적화된 Normal Float 4
</span>    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="c1"># 연산은 bf16으로 수행 (속도/성능 보존)
</span>    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># 양자화 상수를 한 번 더 양자화 (메모리 추가 절약)
</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-70b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>이 설정은 파인튜닝 기법인 <strong>QLoRA(Quantized LoRA)</strong>의 기반이 되며, 소비자용 GPU(RTX 3090/4090)에서도 LLM 학습을 가능하게 만든 일등공신이다.</p> <hr/> <h2 id="2-flash-attention-2-메모리-대역폭-병목-해소">2. Flash Attention 2: 메모리 대역폭 병목 해소</h2> <p>Transformer 구조의 핵심인 Attention 메커니즘은 입력 시퀀스 길이($N$)의 제곱($N^2$)에 비례하여 연산량과 메모리 사용량이 증가한다. 문장이 길어질수록 급격히 느려지는 이유다.</p> <p><strong>Flash Attention</strong>은 GPU 메모리(HBM) 접근 횟수를 획기적으로 줄여(IO-Aware), 긴 문맥 처리 속도를 2배 이상 빠르게 만든다.</p> <h3 id="적용-방법">적용 방법</h3> <p>복잡한 구현 없이 <code class="language-plaintext highlighter-rouge">use_flash_attention_2=True</code> 옵션만 켜면 된다. (단, Ampere 아키텍처 이상의 GPU 필요: A100, A10, RTX 30/40 시리즈)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> 
    <span class="n">attn_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">flash_attention_2</span><span class="sh">"</span> <span class="c1"># 핵심 옵션
</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p><strong>Benchmark:</strong> 입력 토큰이 4096 이상일 때, 일반 Attention 대비 <strong>2~3배의 속도 향상</strong>과 메모리 절약 효과를 볼 수 있다. RAG(검색 증강 생성)처럼 긴 문서를 다루는 서비스에서는 필수 옵션이다.</p> </blockquote> <hr/> <h2 id="3-kv-cache-했던-계산-또-하지-않기">3. KV Cache: 했던 계산 또 하지 않기</h2> <p>LLM은 다음 토큰을 예측할 때, 이전의 모든 토큰 정보를 다시 참조한다. 매번 처음부터 다시 계산하는 것은 엄청난 낭비다. <strong>Key-Value (KV) Cache</strong>는 이전 토큰들의 연산 결과(Key, Value)를 메모리에 저장해두고 재사용하는 기술이다.</p> <h3 id="-best-practice-use_cachetrue">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">use_cache=True</code></h3> <p>대부분의 모델에서 기본값은 <code class="language-plaintext highlighter-rouge">True</code>이지만, 학습(Training) 시에는 메모리 절약을 위해 <code class="language-plaintext highlighter-rouge">False</code>로 끄는 경우가 많다. 따라서 <strong>추론(Inference) 단계에서는 반드시 켜져 있는지 확인</strong>해야 한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. 모델 설정에서 확인
</span><span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># 2. generation 호출 시 명시
</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span> 
<span class="p">)</span>
</code></pre></div></div> <p>KV Cache를 끄면 문장이 길어질수록 생성 속도가 선형적으로 느려지지만, 켜면 문장 길이에 상관없이 일정한 속도(O(1) per token)를 유지할 수 있다.</p> <hr/> <h2 id="series-conclusion-llm-엔지니어링의-여정">Series Conclusion: LLM 엔지니어링의 여정</h2> <p>총 5편에 걸쳐 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리를 활용한 LLM 엔지니어링 핵심 기술을 살펴보았다.</p> <ol> <li><strong>Tokenizer:</strong> Rust 기반 가속과 Dynamic Padding으로 입력 파이프라인 최적화.</li> <li><strong>Model Loading:</strong> <code class="language-plaintext highlighter-rouge">device_map</code>과 <code class="language-plaintext highlighter-rouge">fp16</code>으로 OOM 없는 로딩.</li> <li><strong>Inference:</strong> <code class="language-plaintext highlighter-rouge">GenerationConfig</code>와 <code class="language-plaintext highlighter-rouge">Streamer</code>로 상용 수준의 서비스 구현.</li> <li><strong>Training:</strong> <code class="language-plaintext highlighter-rouge">Trainer</code>와 <code class="language-plaintext highlighter-rouge">Callback</code>으로 안정적인 학습 파이프라인 구축.</li> <li><strong>Optimization:</strong> <code class="language-plaintext highlighter-rouge">Quantization</code>과 <code class="language-plaintext highlighter-rouge">Flash Attention</code>으로 극한의 효율성 달성.</li> </ol> <p>이 시리즈가 단순한 코드 복사-붙여넣기를 넘어, <strong>“왜 이렇게 설정해야 하는가?”</strong>에 대한 엔지니어링적 통찰을 제공했기를 바란다. 이제 여러분의 데이터를 모델에 태워볼 차례다. Happy Engineering!</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="quantization"/><category term="optimization"/><category term="cuda"/><summary type="html"><![CDATA[BitsAndBytes를 활용한 4bit 양자화(QLoRA) 원리와 Flash Attention 2 적용법. KV Cache의 중요성까지, 모델 성능을 극한으로 끌어올리는 최적화 기법 총정리.]]></summary></entry><entry><title type="html">[Transformers] LLM 성능의 시작점, Tokenizer 최적화 및 활용 전략</title><link href="https://ssolllll.github.io/blog/2025/transformers-optimization/" rel="alternate" type="text/html" title="[Transformers] LLM 성능의 시작점, Tokenizer 최적화 및 활용 전략"/><published>2025-12-07T18:00:00+00:00</published><updated>2025-12-07T18:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/transformers-optimization</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/transformers-optimization/"><![CDATA[<p>LLM 엔지니어링에서 모델(Model) 아키텍처나 양자화(Quantization)에는 많은 관심을 갖지만, 정작 데이터가 모델로 들어가는 첫 관문인 <strong>Tokenizer</strong>의 효율성은 간과하는 경우가 많다.</p> <p>하지만 수십 기가바이트의 텍스트를 처리하는 Pre-training/Fine-tuning 단계나, 실시간으로 요청을 처리하는 Serving 단계에서 비효율적인 Tokenizer 사용은 전체 파이프라인의 <strong>심각한 병목(Bottleneck)</strong>이 될 수 있다.</p> <p>이번 포스트에서는 <code class="language-plaintext highlighter-rouge">transformers</code> 라이브러리의 Tokenizer를 엔지니어링 관점에서 100% 활용하는 세 가지 핵심 전략을 다룬다.</p> <hr/> <h2 id="1-fast-tokenizer-rust의-속도를-빌려라">1. Fast Tokenizer: Rust의 속도를 빌려라</h2> <p>Hugging Face의 <code class="language-plaintext highlighter-rouge">transformers</code>는 두 가지 타입의 토크나이저를 제공한다.</p> <ol> <li><strong>Python Tokenizer:</strong> 순수 파이썬 구현체. 느리다.</li> <li><strong>Fast Tokenizer:</strong> Rust로 구현된 <code class="language-plaintext highlighter-rouge">tokenizers</code> 라이브러리 래퍼(Wrapper). 매우 빠르다.</li> </ol> <p>대부분 <code class="language-plaintext highlighter-rouge">AutoTokenizer.from_pretrained()</code>를 호출하면 자동으로 Fast 버전을 가져오지만, 명시적으로 확인하고 사용하는 습관이 중요하다.</p> <h3 id="-성능-차이-벤치마크">⚡ 성능 차이 벤치마크</h3> <p>수백만 건의 데이터를 전처리할 때, 이 속도 차이는 데이터 로딩 시간(Data Loading Time)을 몇 시간에서 몇 분으로 줄여준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The quick brown fox jumps over the lazy dog.</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">1000</span>

<span class="c1"># 1. Slow Tokenizer (Python)
</span><span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">slow_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Slow: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> sec</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. Fast Tokenizer (Rust)
</span><span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">fast_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Fast: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> sec</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 결과 예시:
# Slow: 1.2031 sec
# Fast: 0.1054 sec (약 10배 이상 빠름)
</span></code></pre></div></div> <blockquote> <p><strong>Tip:</strong> <code class="language-plaintext highlighter-rouge">FastTokenizer</code>는 병렬 처리(Parallelism) 기능도 내장하고 있어, <code class="language-plaintext highlighter-rouge">batched=True</code> 옵션과 함께 사용 시 CPU 코어를 최대한 활용한다.</p> </blockquote> <hr/> <h2 id="2-dynamic-padding-불필요한-연산량-줄이기">2. Dynamic Padding: 불필요한 연산량 줄이기</h2> <p>LLM은 배처(Batch) 처리를 위해 입력 시퀀스의 길이를 통일해야 한다. 이때 부족한 부분을 채우는 것이 <strong>Padding</strong>이다.</p> <h3 id="-bad-practice-고정-길이-padding">🔴 Bad Practice: 고정 길이 Padding</h3> <p>모든 데이터를 모델의 <code class="language-plaintext highlighter-rouge">max_length</code>(예: 2048)에 맞춰 패딩하면, “Hello” 같은 짧은 문장도 2043개의 패딩 토큰(<code class="language-plaintext highlighter-rouge">pad_token</code>)을 계산해야 한다. 이는 GPU 메모리와 연산력의 낭비다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 비효율적인 방식: 무조건 max_length까지 늘림
</span><span class="n">tokenized</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">],</span> 
    <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">max_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="c1"># 결과: [Hello, PAD, PAD, ..., PAD] -&gt; 불필요한 연산 발생
</span></code></pre></div></div> <h3 id="-best-practice-dynamic-padding-datacollator">🟢 Best Practice: Dynamic Padding (DataCollator)</h3> <p>배치(Batch) 내에서 <strong>가장 긴 문장의 길이에 맞춰서</strong> 패딩하는 방식이다. 데이터셋 전체가 아닌, 미니 배치 단위로 길이를 맞추므로 연산량이 획기적으로 줄어든다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorWithPadding</span>

<span class="c1"># 데이터셋 단계에서는 패딩하지 않음 (truncation만 적용)
</span><span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># DataLoader가 배치를 만들 때 패딩 수행
</span><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Trainer나 DataLoader에 collator 전달
# trainer = Trainer(..., data_collator=data_collator)
</span></code></pre></div></div> <p>이 방식을 적용하면 학습 속도가 데이터 길이에 따라 <strong>20~30% 이상 향상</strong>될 수 있다.</p> <hr/> <h2 id="3-chat-template-프롬프트-엔지니어링의-표준화">3. Chat Template: 프롬프트 엔지니어링의 표준화</h2> <p>LLM마다 요구하는 프롬프트 포맷이 다르다. (Llama-2의 <code class="language-plaintext highlighter-rouge">[INST]</code>, ChatML의 <code class="language-plaintext highlighter-rouge">&lt;|im_start|&gt;</code>, Alpha의 <code class="language-plaintext highlighter-rouge">Human:</code> 등) 이를 하드코딩으로 문자열을 합치는 방식은 모델 교체 시 코드를 전면 수정해야 하는 리스크가 있다.</p> <h3 id="apply_chat_template-활용"><code class="language-plaintext highlighter-rouge">apply_chat_template</code> 활용</h3> <p><code class="language-plaintext highlighter-rouge">transformers</code> 4.34.0부터 도입된 이 기능은 모델의 <code class="language-plaintext highlighter-rouge">tokenizer_config.json</code>에 정의된 템플릿을 자동으로 적용해준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">RAG 시스템의 장점은?</span><span class="sh">"</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">최신 정보를 반영할 수 있다는 점입니다.</span><span class="sh">"</span><span class="p">},</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">단점은?</span><span class="sh">"</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># 모델에 맞는 포맷으로 자동 변환
</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span> 
    <span class="n">tokenize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># Llama-2인 경우: &lt;s&gt;[INST] RAG 시스템의 장점은? [/INST] 최신 정보를 ... &lt;/s&gt;&lt;s&gt;[INST] 단점은? [/INST]
# Mistral인 경우: ... (해당 모델 포맷 자동 적용)
</span></code></pre></div></div> <p>이 기능을 사용하면 <strong>모델 의존성(Model Dependency)을 코드에서 분리</strong>할 수 있어, 여러 모델을 실험하거나 A/B 테스트를 진행할 때 유지보수성이 극대화된다.</p> <hr/> <h2 id="결론">결론</h2> <p>Tokenizer는 단순히 자연어 처리를 위한 전처리 도구가 아니다.</p> <ul> <li><strong>Rust 기반 Fast Tokenizer</strong>로 CPU 병목을 해소하고,</li> <li><strong>Dynamic Padding</strong>으로 GPU 연산 효율을 높이며,</li> <li><strong>Chat Template</strong>으로 코드의 유연성을 확보하는 것.</li> </ul> <p>이것이 LLM 엔지니어가 갖춰야 할 “입력 파이프라인 최적화”의 기본기다. 다음 포스트에서는 이렇게 처리된 데이터를 거대 모델에 효율적으로 적재하는 <strong>Model Loading 전략</strong>에 대해 다루겠다.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="transformers"/><category term="llm"/><category term="optimization"/><category term="nlp"/><summary type="html"><![CDATA[Hugging Face Tokenizer의 Rust 기반 가속, Dynamic Padding을 통한 연산량 절감, 그리고 Chat Template을 활용한 프롬프트 엔지니어링 표준화 가이드.]]></summary></entry><entry><title type="html">[Python] collections 모듈 심층 분석</title><link href="https://ssolllll.github.io/blog/2025/python-package-collections/" rel="alternate" type="text/html" title="[Python] collections 모듈 심층 분석"/><published>2025-12-05T10:00:00+00:00</published><updated>2025-12-05T10:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/python-package-collections</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/python-package-collections/"><![CDATA[<p>LLM Model 서비스를 개발하다 보면 모델 자체의 추론 속도뿐만 아니라, 전후처리 파이프라인의 효율성이 전체 시스템 성능을 좌우하는 경우를 자주 마주합니다.ㄴ</p> <p>특히 Python의 기본 자료구조(<code class="language-plaintext highlighter-rouge">list</code>, <code class="language-plaintext highlighter-rouge">dict</code>)는 범용적이지만, <strong>실시간 스트리밍 처리가 필요한 Chat Completions API</strong>나 <strong>수십만 개의 Vector Search 결과를 다루는 RAG(Retrieval-Augmented Generation)</strong> 과정에서는 미세한 성능 저하와 불필요한 메모리 점유의 원인이 됩니다.</p> <p><code class="language-plaintext highlighter-rouge">collections</code> 모듈을 단순한 편의 도구가 아닌, <strong>엔지니어링 관점의 최적화 도구</strong>로 재해석하고 LLM 실무 적용 사례를 기록합니다.</p> <hr/> <h2 id="1-deque-context-window-관리와-streaming-buffer">1. deque: Context Window 관리와 Streaming Buffer</h2> <p>LLM은 입력 가능한 토큰 길이에 물리적 한계가 있다. 챗봇 구현 시 가장 오래된 대화를 삭제하고 새 대화를 넣는 <strong>Sliding Window</strong> 전략이 필수적이다. 또한, SSE(Server-Sent Events) 스트리밍 시 문장 단위 처리를 위한 버퍼링에도 큐 구조가 필요합니다.</p> <h3 id="-bad-practice-list-사용">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">list</code> 사용</h3> <p>Python의 <code class="language-plaintext highlighter-rouge">list</code>는 동적 배열(Dynamic Array)이다. <code class="language-plaintext highlighter-rouge">pop(0)</code> 연산은 첫 번째 요소를 제거한 뒤, 나머지 <strong>모든 요소의 인덱스를 한 칸씩 당겨오는(Shift)</strong> 작업을 수행합니다.</p> <ul> <li><strong>시간 복잡도:</strong> $O(N)$</li> <li><strong>문제점:</strong> 대화 턴(Turn)이 길어질수록, 동시 접속자가 늘어날수록 CPU 사이클을 낭비하며, GC(Garbage Collection) 압력을 높인다.</li> </ul> <h3 id="-best-practice-deque-circular-buffer">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">deque</code> (Circular Buffer)</h3> <p><code class="language-plaintext highlighter-rouge">collections.deque</code>는 Linked List 기반의 양방향 큐다. 양 끝단에서의 삽입/삭제가 메모리 이동 없이 포인터 변경만으로 이루어진다.</p> <p><img src="/assets/img/deque_vs_list_memory.png" alt="Deque vs List Memory Structure"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">class</span> <span class="nc">ContextManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">max_turns</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># maxlen을 지정하면 꽉 찼을 때 append 시 
</span>        <span class="c1"># 자동으로 반대편 데이터가 O(1)로 삭제됨 (Memory Allocation 발생 안 함)
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_turns</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_dialogue</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">assistant</span><span class="p">):</span>
        <span class="c1"># 튜플로 저장하여 불변성(Immutability) 보장 및 메모리 절약
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="n">user</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="n">assistant</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">get_prompt_context</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># LLM API 요청 직전에만 list로 변환 (변환 비용은 무시할 수준)
</span>        <span class="k">return</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">}</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">]</span>
</code></pre></div></div> <blockquote> <p><strong>Key Engineering Point:</strong> <code class="language-plaintext highlighter-rouge">maxlen</code>을 설정한 <code class="language-plaintext highlighter-rouge">deque</code>는 링 버퍼(Ring Buffer)처럼 동작하여, 오래된 데이터를 삭제하는 코드를 별도로 작성할 필요가 없어 코드가 간결해지고 실수가 줄어든다.</p> </blockquote> <hr/> <h2 id="2-namedtuple-대규모-vector-search-결과의-메모리-경량화">2. namedtuple: 대규모 Vector Search 결과의 메모리 경량화</h2> <p>RAG(검색 증강 생성) 시스템에서는 Vector DB로부터 수천 개의 청크(Chunk)를 검색하고, 이를 Reranking 하는 과정을 거친다. 이때 각 검색 결과를 <code class="language-plaintext highlighter-rouge">dict</code>로 관리하는 것은 메모리 비효율적이다.</p> <h3 id="-bad-practice-dict-리스트-사용">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">dict</code> 리스트 사용</h3> <p>Python의 <code class="language-plaintext highlighter-rouge">dict</code>는 해시 테이블 구조로, 데이터를 저장할 때 Key 값 자체와 해시 충돌 방지를 위한 추가 공간을 점유한다. 객체 하나당 오버헤드가 크다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 일반적인 딕셔너리 구조
</span><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">doc_1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.89</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="p">}</span> 
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="p">]</span>
<span class="c1"># 수만 건의 검색 결과 처리 시 메모리 사용량 급증
</span></code></pre></div></div> <h3 id="-best-practice-namedtuple-활용">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">namedtuple</code> 활용</h3> <p><code class="language-plaintext highlighter-rouge">collections.namedtuple</code>은 내부적으로 튜플(Tuple) 구조를 사용하되, 필드명으로 접근할 수 있게 해준다. <code class="language-plaintext highlighter-rouge">__dict__</code> 속성을 가지지 않아 메모리 사용량이 현저히 적다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># 검색 결과 구조체 정의
</span><span class="n">SearchResult</span> <span class="o">=</span> <span class="nf">namedtuple</span><span class="p">(</span><span class="sh">"</span><span class="s">SearchResult</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># 메모리 사용량 비교 실험
</span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">}</span>
<span class="n">n</span> <span class="o">=</span> <span class="nc">SearchResult</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dict Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="nf">getsizeof</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span>       <span class="c1"># 예: 232 bytes
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NamedTuple Size: </span><span class="si">{</span><span class="n">sys</span><span class="p">.</span><span class="nf">getsizeof</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 예: 72 bytes
# -&gt; 약 3배 이상의 메모리 절약 효과
</span></code></pre></div></div> <p><strong>적용 시나리오:</strong></p> <ul> <li>Vector DB에서 가져온 1차 후보군(Top-K 100~1000개)을 메모리에 올릴 때.</li> <li>Reranker 모델에 배치(Batch)로 데이터를 넘기기 전 전처리 과정.</li> </ul> <hr/> <h2 id="3-defaultdict-문서-청크chunk-그룹핑-가속화">3. defaultdict: 문서 청크(Chunk) 그룹핑 가속화</h2> <p>긴 문서를 처리할 때, 여러 개의 청크로 나뉘어 처리된 결과를 다시 원본 문서 ID 기준으로 합쳐야 하는 경우가 있다(Map-Reduce 패턴).</p> <h3 id="-bad-practice-if-key-in-dict-체크">🔴 Bad Practice: <code class="language-plaintext highlighter-rouge">if key in dict</code> 체크</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
    <span class="n">doc_id</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">doc_id</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">doc_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span><span class="p">[</span><span class="n">doc_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results</span><span class="p">[</span><span class="n">doc_id</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">summary</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <p>이 방식은 매 반복마다 Key 존재 여부를 해싱하여 검사하므로 불필요한 연산이 포함된다.</p> <h3 id="-best-practice-defaultdict">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">defaultdict</code></h3> <p><code class="language-plaintext highlighter-rouge">collections.defaultdict</code>는 Key가 없을 때의 초기화 로직을 C 레벨에서 처리하므로, Python 레벨의 분기문(Branching)을 제거하여 루프 속도를 높인다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="c1"># 원본 문서 ID별로 요약본을 모으는 파이프라인
</span><span class="n">aggregated_data</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
    <span class="c1"># Key 검사 로직 삭제 -&gt; 코드 가독성 향상 및 속도 최적화
</span>    <span class="n">aggregated_data</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">doc_id</span><span class="sh">'</span><span class="p">]].</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="sh">'</span><span class="s">summary</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="4-chainmap-llm-하이퍼파라미터-계층-관리">4. ChainMap: LLM 하이퍼파라미터 계층 관리</h2> <p>LLM 추론 시 <code class="language-plaintext highlighter-rouge">Temperature</code>, <code class="language-plaintext highlighter-rouge">Top_p</code>, <code class="language-plaintext highlighter-rouge">Max_tokens</code> 등의 설정은 <strong>[기본 설정] -&gt; [유저 설정] -&gt; [프롬프트별 임시 설정]</strong> 순으로 우선순위를 가진다. 이를 <code class="language-plaintext highlighter-rouge">dict.update()</code>로 매번 새로운 딕셔너리를 생성하여 병합하는 것은 비효율적이다.</p> <h3 id="-best-practice-chainmap">🟢 Best Practice: <code class="language-plaintext highlighter-rouge">ChainMap</code></h3> <p><code class="language-plaintext highlighter-rouge">collections.ChainMap</code>은 여러 딕셔너리를 실제로 병합(Copy)하지 않고, 연결된 리스트처럼 논리적으로만 병합하여 보여준다. 조회 시 앞쪽 딕셔너리부터 탐색한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">ChainMap</span>

<span class="n">default_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">}</span>
<span class="n">user_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span>  <span class="c1"># 유저가 온도를 낮춤
</span><span class="n">request_override</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1024</span><span class="p">}</span> <span class="c1"># 이번 요청만 길게
</span>
<span class="c1"># 새로운 딕셔너리 생성 없이 논리적 뷰만 생성 (Zero-copy)
</span><span class="n">final_config</span> <span class="o">=</span> <span class="nc">ChainMap</span><span class="p">(</span><span class="n">request_override</span><span class="p">,</span> <span class="n">user_config</span><span class="p">,</span> <span class="n">default_config</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">])</span> <span class="c1"># 0.5 (user_config 적용)
</span><span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">max_tokens</span><span class="sh">'</span><span class="p">])</span>  <span class="c1"># 1024 (request_override 적용)
</span><span class="nf">print</span><span class="p">(</span><span class="n">final_config</span><span class="p">[</span><span class="sh">'</span><span class="s">top_p</span><span class="sh">'</span><span class="p">])</span>       <span class="c1"># 0.9 (default_config 적용)
</span></code></pre></div></div> <p>이 방식은 요청마다 거대한 설정 객체를 복사할 필요가 없어 <strong>High-Throughput API 서버</strong>에서 GC 오버헤드를 줄이는 데 기여한다.</p> <hr/> <h2 id="결론-micro-optimization이-모여-시스템-안정성을-만든다">결론: Micro-Optimization이 모여 시스템 안정성을 만든다</h2> <p>Python은 느리다는 편견이 있지만, 내장된 <code class="language-plaintext highlighter-rouge">collections</code> 모듈을 적재적소에 활용하면 C로 구현된 내부 최적화의 혜택을 그대로 누릴 수 있다.</p> <ul> <li><strong>Queueing/History:</strong> <code class="language-plaintext highlighter-rouge">deque</code></li> <li><strong>Data Object:</strong> <code class="language-plaintext highlighter-rouge">namedtuple</code></li> <li><strong>Grouping:</strong> <code class="language-plaintext highlighter-rouge">defaultdict</code></li> <li><strong>Config Merging:</strong> <code class="language-plaintext highlighter-rouge">ChainMap</code></li> </ul> <p>LLM 서비스는 텍스트라는 비정형 데이터를 대량으로 다루는 만큼, 이러한 자료구조의 선택이 <strong>Latency(지연 시간)</strong>와 <strong>Memory Footprint(메모리 사용량)</strong>에 미치는 영향이 생각보다 크다. 화려한 모델 튜닝 이전에, 견고한 데이터 파이프라인 구축이 선행되어야 함을 잊지 말자.</p>]]></content><author><name></name></author><category term="engineering"/><category term="python"/><category term="llm"/><category term="optimization"/><category term="backend"/><summary type="html"><![CDATA[단순한 문법 소개가 아닌, 나에게 있어 필요한 collections 모듈 분석.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://ssolllll.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://ssolllll.github.io/blog/2025/plotly</id><content type="html" xml:base="https://ssolllll.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://ssolllll.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://ssolllll.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://ssolllll.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://ssolllll.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://ssolllll.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://ssolllll.github.io/blog/2024/tabs</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="ccc385c2-8267-4563-b996-63e602cfb62a" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="ccc385c2-8267-4563-b996-63e602cfb62a" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="6a3f394d-06bf-485c-b66e-9b8469937f0d" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="6a3f394d-06bf-485c-b66e-9b8469937f0d" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="4d835c35-3564-4256-8f29-7f9be2ee17dc" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="4d835c35-3564-4256-8f29-7f9be2ee17dc" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://ssolllll.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://ssolllll.github.io/blog/2024/typograms</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://ssolllll.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://ssolllll.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://ssolllll.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://ssolllll.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://ssolllll.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>